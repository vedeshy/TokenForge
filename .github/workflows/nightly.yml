name: Nightly Benchmark

on:
  schedule:
    - cron: '0 0 * * *'  # Run at midnight every day
  workflow_dispatch:  # Allow manual triggering

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  CUDA_VERSION: '12.2'
  GPU_SKU: 'L4'

jobs:
  benchmark:
    runs-on: [self-hosted, gpu]
    steps:
      - uses: actions/checkout@v3

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.21'

      - name: Build API
        run: cd api && go build -v -o ../bin/api

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push vLLM worker image
        uses: docker/build-push-action@v4
        with:
          context: ./workers/worker-vllm
          push: true
          tags: ${{ env.REGISTRY }}/${{ github.repository }}/worker-vllm:nightly
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            CUDA_VERSION=${{ env.CUDA_VERSION }}

      - name: Build and push Transformers worker image
        uses: docker/build-push-action@v4
        with:
          context: ./workers/worker-transformers
          push: true
          tags: ${{ env.REGISTRY }}/${{ github.repository }}/worker-transformers:nightly
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            CUDA_VERSION=${{ env.CUDA_VERSION }}

      - name: Start infrastructure
        run: docker compose up -d postgres minio prometheus grafana

      - name: Wait for services
        run: sleep 10

      - name: Run API server
        run: ./bin/api &

      - name: Wait for API server
        run: sleep 5

      - name: Create benchmark configs
        run: |
          # Regular benchmark
          cat > /tmp/nightly_bench.yaml << EOF
          model: meta-llama/Llama-3-8b-instruct
          runtimes: [vllm, transformers]
          workloads:
            - name: qa-short
              qps: 2
              duration_s: 60
              prompt_len: 256
              gen_tokens: 128
            - name: code-long
              qps: 1
              duration_s: 120
              prompt_len: 512
              gen_tokens: 256
          EOF
          
          # Streaming benchmark
          cat > /tmp/streaming_bench.yaml << EOF
          model: meta-llama/Llama-3-8b-instruct
          runtimes: [vllm, transformers]
          workloads:
            - name: qa-streaming
              qps: 1
              duration_s: 60
              prompt_len: 256
              gen_tokens: 128
              stream: true
          EOF

      - name: Run benchmarks
        run: |
          # Run regular benchmark
          export RUN_ID=nightly_$(date +%Y%m%d)
          python harness/run_bench.py --run-id $RUN_ID --config /tmp/nightly_bench.yaml
          
          # Run streaming benchmark
          export STREAM_RUN_ID=streaming_$(date +%Y%m%d)
          python harness/run_bench.py --run-id $STREAM_RUN_ID --config /tmp/streaming_bench.yaml

      - name: Collect artifacts
        run: |
          mkdir -p artifacts
          cp /tmp/nightly_*/report.html artifacts/
          cp /tmp/nightly_*/summary.csv artifacts/
          cp /tmp/nightly_*/raw.json artifacts/
          cp /tmp/streaming_*/report.html artifacts/streaming_report.html
          cp /tmp/streaming_*/summary.csv artifacts/streaming_summary.csv
          cp /tmp/streaming_*/raw.json artifacts/streaming_raw.json

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: artifacts/

      - name: Comment on commit
        uses: peter-evans/commit-comment@v2
        with:
          body: |
            ## Nightly Benchmark Results
            
            Benchmark completed successfully! [View full report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            
            ### Environment
            - GPU: ${{ env.GPU_SKU }}
            - CUDA: ${{ env.CUDA_VERSION }}
            - Date: $(date +%Y-%m-%d)
            
            ### Summary
            Two benchmark suites were run:
            - Standard inference benchmark
            - Streaming inference benchmark
            
            See attached artifacts for detailed results including:
            - Latency metrics (p50, p95, p99)
            - Token generation rates
            - Time to first token (streaming)
            - Inter-token latency (streaming)
